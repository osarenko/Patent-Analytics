{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-20T23:38:01.606464Z",
     "start_time": "2025-06-20T23:37:47.074023Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "import certifi\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "#Patent Data Retrieval\n",
    "# ----------------------------\n",
    "patent_numbers = ['US2020057781A1',\t'US10664487B2',\t'US10474562B2',\t'US9910911B2']\n",
    " # Add more patent numbers here as needed\n",
    "claim_patent_mapping = []\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "patents = []\n",
    "claim_texts = []\n",
    "\n",
    "for idx, pn in enumerate(patent_numbers):\n",
    "    try:\n",
    "        url = f\"https://patents.google.com/patent/{pn}\"\n",
    "        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req, context=ssl_context, timeout=10).read()\n",
    "        soup = BeautifulSoup(webpage, 'lxml')\n",
    "\n",
    "        # Extract claims\n",
    "        claims_section = soup.find('section', {'itemprop': 'claims'})\n",
    "        if not claims_section:\n",
    "            print(f\"Skipping {pn}: No claims section\")\n",
    "            continue\n",
    "\n",
    "        claims = [c.get_text(' ', strip=True) for c in claims_section.find_all('div', class_='claim-text')]\n",
    "        if not claims:\n",
    "            print(f\"Skipping {pn}: Empty claims\")\n",
    "            continue\n",
    "\n",
    "        claim_texts.extend(claims)\n",
    "        claim_patent_mapping.extend([pn] * len(claims))\n",
    "        patents.append({\n",
    "            'number': pn,\n",
    "            'claims': claims,\n",
    "            'abstract': soup.find('meta', attrs={'name':'DC.description'})['content']\n",
    "                        if soup.find('meta', attrs={'name':'DC.description'}) else None\n",
    "        })\n",
    "\n",
    "        # Progress tracking\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == len(patent_numbers):\n",
    "            print(f\"Processed {idx+1}/{len(patent_numbers)} patents\")\n",
    "\n",
    "        # Rate limiting\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {pn}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Save patent numbers, claims, and abstract for backup\n",
    "\n",
    "claim_texts = [str(t) for t in claim_texts]  # Ensure string format\n",
    "\n",
    "assert len(claim_texts) > 0, \"No claims loaded from file\"\n",
    "print(f\"Loaded {len(claim_texts)} patent claims\")\n",
    "\n",
    "with open('patents.pkl', 'wb') as f:\n",
    "    pickle.dump(patents, f)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Batched Processing Pipeline (FIXED)\n",
    "# ----------------------------\n",
    "def batch_tokenize(texts, tokenizer, batch_size=64):\n",
    "    #Tokenize text in batches with consistent padding\n",
    "    if not texts:\n",
    "        raise ValueError(\"Empty input list - check your claims.pkl file\")\n",
    "\n",
    "    # Calculate global max length\n",
    "    max_length = 0\n",
    "    for txt in tqdm(texts, desc=\"Calculating max length\"):\n",
    "        encoded = tokenizer.encode(txt, truncation=True, add_special_tokens=True)\n",
    "        max_length = max(max_length, len(encoded))\n",
    "    max_length = min(max_length, 512)\n",
    "\n",
    "    all_inputs = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size),\n",
    "                 desc=\"Tokenizing\",\n",
    "                 unit=\"batch\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',  # Crucial fix\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        all_inputs['input_ids'].append(inputs['input_ids'])\n",
    "        all_inputs['attention_mask'].append(inputs['attention_mask'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.cat(all_inputs['input_ids'], dim=0),\n",
    "        'attention_mask': torch.cat(all_inputs['attention_mask'], dim=0)\n",
    "    }\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "print(f\"\\nModel loaded on {device}...\")\n",
    "\n",
    "# Get properly padded inputs\n",
    "inputs = batch_tokenize(claim_texts, tokenizer)\n",
    "\n",
    "\n",
    "def batched_inference(model, inputs, batch_size=32):\n",
    "    \"\"\"Run model inference with proper batching\"\"\"\n",
    "    embeddings = []\n",
    "    total_batches = len(inputs['input_ids']) // batch_size + 1\n",
    "\n",
    "    for i in tqdm(range(0, len(inputs['input_ids']), batch_size),\n",
    "                 desc=\"Generating Embeddings\",\n",
    "                 total=total_batches,\n",
    "                 unit=\"batch\"):\n",
    "        batch = {\n",
    "            'input_ids': inputs['input_ids'][i:i+batch_size].to(device),\n",
    "            'attention_mask': inputs['attention_mask'][i:i+batch_size].to(device)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = batched_inference(model, inputs)\n",
    "\n",
    "# ----------------------------\n",
    "#Data Cleansing\n",
    "# ----------------------------\n",
    "valid_mask = ~np.isnan(embeddings).any(axis=1) & ~np.isinf(embeddings).any(axis=1)\n",
    "claim_patent_mapping_clean = [claim_patent_mapping[i] for i in np.where(valid_mask)[0]]\n",
    "clean_embeddings = embeddings[valid_mask]\n",
    "\n",
    "norms = np.linalg.norm(clean_embeddings, axis=1)\n",
    "median_norm = np.median(norms)\n",
    "clean_embeddings = clean_embeddings[norms > median_norm * 0.01]\n",
    "\n",
    "print(f\"\\nCleaning report:\")\n",
    "print(f\"Original: {embeddings.shape[0]} vectors\")\n",
    "print(f\"Final: {clean_embeddings.shape[0]} vectors\")\n",
    "print(f\"Removed: {embeddings.shape[0] - clean_embeddings.shape[0]} vectors\")\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = reducer.fit_transform(clean_embeddings)\n",
    "\n",
    "with open('embeddings_2d.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_2d, f)\n",
    "\n",
    "with open('claim_patent_mapping_clean.pkl' 'wb') as f:\n",
    "    pickle.dump(claim_patent_mapping_clean, f)\n",
    "\n",
    "# Find points far from cluster centers\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=50).fit(embeddings_2d)\n",
    "distances, indices = nbrs.kneighbors(embeddings_2d)\n",
    "mean_distances = distances.mean(axis=1)\n",
    "\n",
    "# Use percentile threshold instead of fixed contamination\n",
    "threshold = np.percentile(mean_distances, 99)\n",
    "outlier_indices = np.where(mean_distances > threshold)[0]\n",
    "outlier_patents = list(set([claim_patent_mapping_clean[i] for i in outlier_indices]))\n",
    "print(f\"Outlier patents: {outlier_patents}\")\n",
    "\n",
    "# Modify alpha values for patent outlier detection\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], s=2, alpha=0.5, label='Normal')\n",
    "plt.scatter(embeddings_2d[outlier_indices,0],\n",
    "            embeddings_2d[outlier_indices,1],\n",
    "            color='red', s=20, label='Outlier Claims')\n",
    "plt.title(\"Patent Claim Landscape with Outliers Highlighted\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on US2020057781A1: HTTP Error 404: Not Found\n",
      "Processed 5/3971 patents\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 59\u001B[39m\n\u001B[32m     56\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mProcessed \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(patent_numbers)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m patents\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     58\u001B[39m     \u001B[38;5;66;03m# Rate limiting\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     62\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpn\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T23:33:06.981942Z",
     "start_time": "2025-06-20T23:33:06.956794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('claims.pkl' 'wb') as f:\n",
    "    pickle.dump(claim_texts, f)"
   ],
   "id": "5933acc3105e947c",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'claims.pklwb'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mclaims.pkl\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mwb\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m      2\u001B[39m     pickle.dump(claim_texts, f)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv1/lib/python3.12/site-packages/IPython/core/interactiveshell.py:327\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    320\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    321\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    322\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    325\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'claims.pklwb'"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
